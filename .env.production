# =================================================
# AI-RAG-Project Production Environment Configuration
# =================================================
# Copy this file to .env and configure for your environment

# =============================
# Core Application Settings
# =============================

# Model Processing Device (auto, cpu, cuda)
MODEL_DEVICE=cuda

# Docker Optimization (should be true for containerized deployment)
DOCKER_PRELOADED_MODELS=true

# Data and Cache Directories
RAG_DATA_DIR=/app/data
RAG_CACHE_DIR=/app/cache

# =============================
# API Keys and External Services
# =============================

# OpenAI API (optional - local embeddings work without this)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Salesforce Integration (optional)
# Required for Salesforce knowledge base integration
SALESFORCE_USERNAME=your_salesforce_username
SALESFORCE_PASSWORD=your_salesforce_password
SALESFORCE_SECURITY_TOKEN=your_salesforce_security_token

# =============================
# System Dependencies
# =============================

# Poppler Path Override (usually auto-detected)
# Only set if you have custom poppler installation
# POPPLER_PATH=/usr/bin

# =============================
# Streamlit Server Configuration
# =============================

STREAMLIT_SERVER_HEADLESS=true
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0
STREAMLIT_SERVER_MAX_UPLOAD_SIZE=200

# Disable file watcher for production (improves performance)
STREAMLIT_SERVER_FILE_WATCHER_TYPE=none

# =============================
# Model Cache Configuration
# =============================

# Hugging Face and Transformers Cache Locations
TRANSFORMERS_CACHE=/app/models/transformers
HF_HOME=/app/models/huggingface
TORCH_HOME=/app/models/torch

# =============================
# GPU Configuration (if applicable)
# =============================

# CUDA Visible Devices (0 for first GPU, 0,1 for first two GPUs)
CUDA_VISIBLE_DEVICES=0

# NVIDIA Container Settings (set automatically by docker-compose)
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# PyTorch CUDA Memory Management
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# CUDA Architecture Support (auto-detected, but can override)
# TORCH_CUDA_ARCH_LIST=6.0;6.1;7.0;7.5;8.0;8.6;9.0

# =============================
# Performance Tuning
# =============================

# Disable CUDA launch blocking for better performance
CUDA_LAUNCH_BLOCKING=0

# Python unbuffered output for better logging
PYTHONUNBUFFERED=1

# OpenMP Thread Limit (prevents over-subscription)
OMP_NUM_THREADS=4

# =============================
# Security Settings
# =============================

# Disable debug mode in production
STREAMLIT_SERVER_ENABLE_CORS=false
STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION=false

# =============================
# Logging and Monitoring
# =============================

# Python Logging Level (DEBUG, INFO, WARNING, ERROR)
PYTHON_LOG_LEVEL=INFO

# Enable/Disable specific loggers
HTTPX_LOG_LEVEL=WARNING
OPENAI_LOG_LEVEL=WARNING

# =============================
# Docker Compose Override Settings
# =============================

# These can be used to override docker-compose.yml settings

# Container Names (if you want to customize)
# AI_RAG_CPU_CONTAINER_NAME=ai-rag-production-cpu
# AI_RAG_GPU_CONTAINER_NAME=ai-rag-production-gpu

# Port Mappings (if you want different ports)
# AI_RAG_CPU_PORT=8501
# AI_RAG_GPU_PORT=8502

# Memory Limits (Docker format: 8g, 16g, etc.)
# AI_RAG_CPU_MEMORY_LIMIT=8g
# AI_RAG_GPU_MEMORY_LIMIT=16g

# =============================
# Deployment Information
# =============================

# Deployment Environment (for monitoring/logging)
DEPLOYMENT_ENV=production

# Application Version (for tracking)
APP_VERSION=2.0

# Build Timestamp (set automatically during build)
# BUILD_TIMESTAMP=2025-01-30T12:00:00Z

# =============================
# Notes for GPU-Enabled Linux Server Deployment
# =============================

# Prerequisites:
# 1. NVIDIA Drivers 470+ installed
# 2. nvidia-container-toolkit installed
# 3. Docker Compose with GPU support
# 4. Sufficient GPU memory (8GB+ recommended)

# Deployment Commands:
# CPU: docker-compose up -d ai-rag-app
# GPU: docker-compose --profile gpu up -d ai-rag-app-gpu

# Monitoring Commands:
# Logs: docker-compose logs -f ai-rag-app-gpu
# GPU Usage: nvidia-smi -l 1
# Container Stats: docker stats