# =================================================
# AI-RAG-Project Production Environment Configuration
# =================================================
# Copy this file to .env and configure for your environment

# =============================
# Core Application Settings
# =============================

# Model Processing Device (auto, cpu, cuda)
MODEL_DEVICE=cuda

# Docker Optimization (should be true for containerized deployment)
DOCKER_PRELOADED_MODELS=true

# Data and Cache Directories
RAG_DATA_DIR=/app/data
RAG_CACHE_DIR=/app/cache

# =============================
# API Keys and External Services
# =============================

# OpenAI API (optional - local embeddings work without this)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Salesforce Integration (optional)
# Required for Salesforce knowledge base integration
SALESFORCE_USERNAME=your_salesforce_username
SALESFORCE_PASSWORD=your_salesforce_password
SALESFORCE_SECURITY_TOKEN=your_salesforce_security_token

# =============================
# System Dependencies
# =============================

# Poppler Path Override (usually auto-detected)
# Only set if you have custom poppler installation
# POPPLER_PATH=/usr/bin

# =============================
# Streamlit Server Configuration
# =============================

STREAMLIT_SERVER_HEADLESS=true
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0
STREAMLIT_SERVER_MAX_UPLOAD_SIZE=200

# Disable file watcher for production (improves performance)
STREAMLIT_SERVER_FILE_WATCHER_TYPE=none

# =============================
# Model Cache Configuration
# =============================

# Hugging Face and Transformers Cache Locations
TRANSFORMERS_CACHE=/app/models/transformers
HF_HOME=/app/models/huggingface
TORCH_HOME=/app/models/torch

# =============================
# GPU Configuration (if applicable)
# =============================

# CUDA Visible Devices (0 for first GPU, 0,1 for first two GPUs)
CUDA_VISIBLE_DEVICES=0

# NVIDIA Container Settings (set automatically by docker-compose)
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# PyTorch CUDA Memory Management (optimized for old CPU compatibility)
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,garbage_collection_threshold:0.6

# GPU-only mode for old CPU compatibility (AMD Phenom II X6 1090T)
# Force all AI models to GPU to avoid "Illegal instruction" errors
SENTENCE_TRANSFORMERS_DEVICE=cuda
TRANSFORMERS_DEVICE=cuda

# CUDA Architecture Support (auto-detected, but can override)
# TORCH_CUDA_ARCH_LIST=6.0;6.1;7.0;7.5;8.0;8.6;9.0

# =============================
# Performance Tuning
# =============================

# Disable CUDA launch blocking for better performance
CUDA_LAUNCH_BLOCKING=0

# Python unbuffered output for better logging
PYTHONUNBUFFERED=1

# Thread limits for old CPU compatibility (prevents instruction set issues)
OMP_NUM_THREADS=1
MKL_NUM_THREADS=1
NUMEXPR_NUM_THREADS=1
OPENBLAS_NUM_THREADS=1
VECLIB_MAXIMUM_THREADS=1

# =============================
# Security Settings
# =============================

# Disable debug mode in production
STREAMLIT_SERVER_ENABLE_CORS=false
STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION=false

# =============================
# Logging and Monitoring
# =============================

# Python Logging Level (DEBUG, INFO, WARNING, ERROR)
PYTHON_LOG_LEVEL=INFO

# Enable/Disable specific loggers
HTTPX_LOG_LEVEL=WARNING
OPENAI_LOG_LEVEL=WARNING

# =============================
# Docker Compose Override Settings
# =============================

# These can be used to override docker-compose.yml settings

# Container Names (if you want to customize)
# AI_RAG_CPU_CONTAINER_NAME=ai-rag-production-cpu
# AI_RAG_GPU_CONTAINER_NAME=ai-rag-production-gpu

# Port Mappings (if you want different ports)
# AI_RAG_CPU_PORT=8501
# AI_RAG_GPU_PORT=8502

# Memory Limits (Docker format: 8g, 16g, etc.)
# AI_RAG_CPU_MEMORY_LIMIT=8g
# AI_RAG_GPU_MEMORY_LIMIT=16g

# =============================
# Deployment Information
# =============================

# Deployment Environment (for monitoring/logging)
DEPLOYMENT_ENV=production

# Application Version (for tracking)
APP_VERSION=2.0

# Build Timestamp (set automatically during build)
# BUILD_TIMESTAMP=2025-01-30T12:00:00Z

# =============================
# Notes for GPU-Enabled Linux Server Deployment
# =============================

# Prerequisites:
# 1. NVIDIA Drivers 470+ installed
# 2. nvidia-container-toolkit installed
# 3. Docker Compose with GPU support
# 4. Sufficient GPU memory (8GB+ recommended)

# Deployment Commands:
# CPU: docker-compose up -d ai-rag-app
# GPU: docker-compose --profile gpu up -d ai-rag-app-gpu

# Monitoring Commands:
# Logs: docker-compose logs -f ai-rag-app-gpu
# GPU Usage: nvidia-smi -l 1
# Container Stats: docker stats

# =============================
# Old CPU Compatibility Configuration  
# =============================
# This section is specifically for systems with old CPUs lacking modern
# instruction sets (AVX, AVX2) such as AMD Phenom II X6 1090T (circa 2010)

# Automatic GPU-only mode enforcement (set by deploy script)
# FORCE_GPU_ONLY=true

# CPU instruction set compatibility notes:
# - AMD Phenom II X6 1090T lacks AVX/AVX2 instruction sets
# - Modern AI libraries (SentenceTransformers, PyTorch) require these instructions  
# - Running AI models on CPU will cause "Illegal instruction" errors
# - Solution: Force all AI processing to GPU (RTX 3080 has 24GB VRAM)

# Expected behavior on old CPU systems:
# 1. CPU compatibility check detects old processor
# 2. GPU-only mode automatically enforced (MODEL_DEVICE=cuda)
# 3. All AI models load exclusively on GPU
# 4. CPU thread limits prevent problematic optimizations
# 5. Application runs successfully avoiding CPU instruction issues

# Troubleshooting old CPU deployment:
# - If "Illegal instruction" errors occur, verify MODEL_DEVICE=cuda
# - Check GPU availability: nvidia-smi
# - Verify container has GPU access: docker run --gpus all nvidia/cuda:12.1-base-ubuntu20.04 nvidia-smi
# - Monitor GPU memory usage during startup: nvidia-smi -l 1
# - For memory issues, reduce model batch sizes or increase GPU memory limits